{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyundaicar_descript_merge_all.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b29709f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]\n",
      "/var/folders/69/bt3_5fns57n_cp5_ydbrhfh80000gn/T/ipykernel_93834/4042269537.py:34: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Qdrant 컬렉션 생성 완료\n",
      "✅ Qdrant에 벡터 및 metadata 업로드 완료!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. 데이터 로딩\n",
    "with open('result_dataset/hyundaicar_descript_merge_all.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "descriptions = [item[\"description\"] for item in data]\n",
    "car_info = [\n",
    "    {\n",
    "        \"car_name\": item[\"car_name\"],\n",
    "        \"category\": item[\"category\"],\n",
    "        \"image_path\": item[\"image_path\"]\n",
    "    } for item in data\n",
    "]\n",
    "\n",
    "# 2. 텍스트 임베딩\n",
    "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
    "embeddings = model.encode(descriptions, show_progress_bar=True)\n",
    "\n",
    "# 3. Qdrant 클라이언트 연결 및 컬렉션 생성 (로컬 파일 DB 모드)\n",
    "QDRANT_PATH = Path(\"qdrant\")  # 원하는 경로로 변경 가능\n",
    "COLLECTION_NAME = \"hyundaicar_embeddings\"\n",
    "EMBEDDING_DIM = embeddings[0].shape[0] if hasattr(embeddings[0], \"shape\") else len(embeddings[0])\n",
    "\n",
    "client = QdrantClient(\n",
    "    path=QDRANT_PATH,\n",
    "    prefer_grpc=False  # 로컬 파일 DB 모드 필수\n",
    ")\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(\n",
    "        size=EMBEDDING_DIM,\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n",
    "print(\"✅ Qdrant 컬렉션 생성 완료\")\n",
    "\n",
    "# 4. 데이터 업로드\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=i,\n",
    "        vector=embeddings[i],\n",
    "        payload=car_info[i]\n",
    "    )\n",
    "    for i in range(len(embeddings))\n",
    "]\n",
    "\n",
    "client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "\n",
    "print(\"✅ Qdrant에 벡터 및 metadata 업로드 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cce2f630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qdrant_client.qdrant_client.QdrantClient"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee4ccd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from tools import TOOLS, image_node, generate_node\n",
    "\n",
    "# 환경변수 로드\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4.o-mini\")\n",
    "\n",
    "def check_route(state):\n",
    "    user_input = state[\"input\"]\n",
    "    if \"이미지\" in user_input or \"image\" in user_input:\n",
    "        return \"image\"\n",
    "    elif \"검색\" in user_input or \"찾아\" in user_input or \"RAG\" in user_input:\n",
    "        return \"rag\"\n",
    "    else:\n",
    "        return \"llm\"\n",
    "\n",
    "def rag_node(state):\n",
    "    query = state[\"input\"]\n",
    "    query_vec = model.encode([query])[0]\n",
    "    hits = client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query_vector=query_vec,\n",
    "        limit=3\n",
    "    )\n",
    "    retrieved = [hit.payload for hit in hits]\n",
    "    state[\"intermediate\"] = f\"RAG 검색 결과: {retrieved}\"\n",
    "    return state\n",
    "\n",
    "def llm_node(state):\n",
    "    response = llm.invoke(state[\"input\"])\n",
    "    state[\"intermediate\"] = response.content\n",
    "    return state\n",
    "\n",
    "def synthesize_response(state):\n",
    "    return {\"output\": state.get(\"intermediate\", \"DB에 있는 이미지 또는 일반 응답\")}\n",
    "\n",
    "def start(state):\n",
    "    if isinstance(state, str):\n",
    "        return {\"input\": state}\n",
    "    return state\n",
    "\n",
    "# 아래 함수들은 이미 tools.py에 정의되어 있다고 가정\n",
    "# from tools import image_node, generate_node\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = StateGraph(dict)  # GraphState 대신 dict로도 충분\n",
    "\n",
    "graph.add_node(\"start\", start)\n",
    "graph.add_node(\"check_route\", check_route)\n",
    "graph.add_node(\"image\", image_node)           # tools.py에서 import\n",
    "graph.add_node(\"generate\", generate_node)     # tools.py에서 import\n",
    "graph.add_node(\"rag\", rag_node)\n",
    "graph.add_node(\"llm\", llm_node)\n",
    "graph.add_node(\"synthesize_response\", synthesize_response)\n",
    "\n",
    "graph.set_entry_point(\"start\")\n",
    "graph.add_edge(\"start\", \"check_route\")\n",
    "\n",
    "# check_route 분기\n",
    "graph.add_conditional_edges(\n",
    "    \"check_route\",\n",
    "    check_route,  # 반드시 문자열 반환 함수\n",
    "    {\n",
    "        \"image\": \"image\",\n",
    "        \"rag\": \"rag\",\n",
    "        \"llm\": \"llm\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# image → generate → synthesize_response\n",
    "graph.add_edge(\"image\", \"generate\")\n",
    "graph.add_edge(\"generate\", \"synthesize_response\")\n",
    "\n",
    "# rag/llm은 바로 synthesize_response로\n",
    "graph.add_edge(\"rag\", \"synthesize_response\")\n",
    "graph.add_edge(\"llm\", \"synthesize_response\")\n",
    "\n",
    "graph.add_edge(\"synthesize_response\", END)\n",
    "\n",
    "final_graph = graph.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5493d61",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidUpdateError",
     "evalue": "At key '__root__': Can receive only one value per step. Use an Annotated key to handle multiple values.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidUpdateError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m img = \u001b[43mfinal_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.draw_mermaid_png()\n\u001b[32m      4\u001b[39m Image(img)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# result = flow.invoke(\"안녕\")\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# print(result)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/babsim/lib/python3.13/site-packages/langgraph/pregel/__init__.py:673\u001b[39m, in \u001b[36mPregel.get_graph\u001b[39m\u001b[34m(self, config, xray)\u001b[39m\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    671\u001b[39m     subgraphs = {}\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmerge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterrupt_after_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterrupt_before_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/babsim/lib/python3.13/site-packages/langgraph/pregel/draw.py:152\u001b[39m, in \u001b[36mdraw_graph\u001b[39m\u001b[34m(config, nodes, specs, input_channels, interrupt_after_nodes, interrupt_before_nodes, trigger_to_nodes, checkpointer, subgraphs, limit)\u001b[39m\n\u001b[32m    150\u001b[39m         trigger_to_sources[trigger].add((src, cond, label))\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# apply writes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m updated_channels = \u001b[43mapply_writes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_next_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrigger_to_nodes\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# prepare next tasks\u001b[39;00m\n\u001b[32m    156\u001b[39m tasks = prepare_next_tasks(\n\u001b[32m    157\u001b[39m     checkpoint,\n\u001b[32m    158\u001b[39m     [],\n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m     updated_channels=updated_channels,\n\u001b[32m    171\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/babsim/lib/python3.13/site-packages/langgraph/pregel/algo.py:299\u001b[39m, in \u001b[36mapply_writes\u001b[39m\u001b[34m(checkpoint, channels, tasks, get_next_version, trigger_to_nodes)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chan, vals \u001b[38;5;129;01min\u001b[39;00m pending_writes_by_channel.items():\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chan \u001b[38;5;129;01min\u001b[39;00m channels:\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mchannels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchan\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m next_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    300\u001b[39m             checkpoint[\u001b[33m\"\u001b[39m\u001b[33mchannel_versions\u001b[39m\u001b[33m\"\u001b[39m][chan] = next_version\n\u001b[32m    301\u001b[39m             \u001b[38;5;66;03m# unavailable channels can't trigger tasks, so don't add them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/babsim/lib/python3.13/site-packages/langgraph/channels/last_value.py:58\u001b[39m, in \u001b[36mLastValue.update\u001b[39m\u001b[34m(self, values)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) != \u001b[32m1\u001b[39m:\n\u001b[32m     54\u001b[39m     msg = create_error_message(\n\u001b[32m     55\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAt key \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: Can receive only one value per step. Use an Annotated key to handle multiple values.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     56\u001b[39m         error_code=ErrorCode.INVALID_CONCURRENT_GRAPH_UPDATE,\n\u001b[32m     57\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.value = values[-\u001b[32m1\u001b[39m]\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mInvalidUpdateError\u001b[39m: At key '__root__': Can receive only one value per step. Use an Annotated key to handle multiple values.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "img = final_graph.get_graph().draw_mermaid_png()\n",
    "Image(img)\n",
    "\n",
    "# result = flow.invoke(\"안녕\")\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
